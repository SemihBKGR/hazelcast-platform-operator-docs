= Persistence, Backup and Restore
:description: Hazelcast supports backing up data to the persistent store and restoring from the cluster data from the backup.

{description}

== Enabling Persistence
--
You need to enable persistence for Hazelcast to work with backup and restore.

. Create the Hazelcast resource:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence.yaml[]
----
<1> Base directory of the backup data.
<2> Cluster recovery policy. More information can be found in xref:backup-restore.adoc#restore-policy[] below.
<3> Size of the PVC. *BE SURE* to calculate total size needed beforehand.
It should be larger than the size of the data in the memory.
Additional backups will also take some storage.

. Apply the resource:
+
[source,bash]
----
kubectl apply -f ./hazelcast.yaml
----

. Check Hazelcast instances are ready:
+
[source,bash]
----
kubectl get pods -l app.kubernetes.io/instance=hazelcast
----
+
[source,bash]
----
NAME          READY   STATUS    RESTARTS   AGE
hazelcast-0   1/1     Running   0          2m
hazelcast-1   1/1     Running   0          1m
hazelcast-2   1/1     Running   0          1m

----

. Check Hazelcast PVCs are created:
+
[source,bash]
----
kubectl get pvc -l app.kubernetes.io/instance=hazelcast
----
+
[source,bash]
----
NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
hot-restart-persistence-hazelcast-0   Bound    pvc-116b4084-a436-4462-b413-511b77df307b   20Gi       RWO            standard       2m
hot-restart-persistence-hazelcast-1   Bound    pvc-a7711b6b-dcbf-45cb-9577-8ce6b1892f2f   20Gi       RWO            standard       1m
hot-restart-persistence-hazelcast-2   Bound    pvc-64314d82-da7a-4e38-bd2d-e770a63dc4e8   20Gi       RWO            standard       1m
----
--

== Take a Single Hot Backup
--
. Create the HotBackup resource:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup.yaml[]
----

. Apply the resource:
+
[source,bash]
----
kubectl apply -f ./hot-backup.yaml
----

--

== Take Scheduled Hot Backup

You can also take backups on schedule using the `spec.schedule` field.
--
. Create the HotBackup resource with `schedule` field filled:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-scheduled.yaml[]
----
+
The field needs to take a valid cron expression. Some examples:
+
[cols="1,1"]
|===
| ```30 10 * * *```
| On 10:30 AM every day

| ```0, 0, 1,15,25 * *```
| On 1st, 15th and 25th each month, midnight

| ```@monthly```
| Once a month, first of the month, midnight
|===


. Apply the resource:
+
[source,bash]
----
kubectl apply -f ./hot-backup-scheduled.yaml
----
--

== Restore from HotBackup

. Delete the Hazelcast cluster:
+
[source,bash]
----
kubectl delete hazelcast hazelcast
----

. Check PVCs are still there:
+
[source,bash]
----
kubectl get pvc -l app.kubernetes.io/instance=hazelcast
----
+
[source,bash]
----
NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
hot-restart-persistence-hazelcast-0   Bound    pvc-116b4084-a436-4462-b413-511b77df307b   20Gi       RWO            standard       6m
hot-restart-persistence-hazelcast-1   Bound    pvc-a7711b6b-dcbf-45cb-9577-8ce6b1892f2f   20Gi       RWO            standard       5m
hot-restart-persistence-hazelcast-2   Bound    pvc-64314d82-da7a-4e38-bd2d-e770a63dc4e8   20Gi       RWO            standard       5m

----

. Reapply the Hazelcast resource:
+
[source,bash]
----
kubectl apply -f ./hazelcast-persistence.yaml
----

. Same PVCs will be bound to the new pods:
+
[source,bash]
----
kubectl get pods -l app.kubernetes.io/instance=hazelcast -ojsonpath='{.items[*].spec.volumes[?(@.name=="hot-restart-persistence")].persistentVolumeClaim.claimName}'

----
+
[source,bash]
----
hot-restart-persistence-hazelcast-0 hot-restart-persistence-hazelcast-1 hot-restart-persistence-hazelcast-2
----

=== Restore Policy

To decide how a cluster should behave when one or more members cannot rejoin after a cluster-wide restart, you can define a cluster recovery policy.
options are:

[cols="1,1"]
|===
| ```FullRecoveryOnly```
| Does not allow partial start of the cluster.

| ```PartialRecoveryMostRecent```
| Allows partial start with the members that have most recent partition table.

| ```PartialRecoveryMostComplete```
| Allows partial start with the member that have most complete partition table.

| ```ForceStart```
| Deletes all data in the cluster persistence store if cluster recovery fails.
More detail can be found in link:https://docs.hazelcast.com/hazelcast/5.1/storage/triggering-force-start[Hazelcast documentation].
|===

== HostPath Support for Persistence

You can also use HostPath to enable persistence.

WARNING: HostPath support is discouraged for the production environments for the reasons
mentioned in the link:https://kubernetes.io/docs/concepts/storage/volumes/#hostpath[Kubernetes documentation]

WARNING: HostPath support expects the size of the cluster to be equal to the number of Kubernetes nodes
and pods are distributed to the nodes equally. You can manage it by setting `topologySpreadContraints` field.

. Create the Hazelcast resource with the `clusterSize` equals to the number of Kubernetes nodes
and give the proper `topologySpreadConstraints`:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-hostpath.yaml[]
----

. Apply the Hazelcast resource
+
[source,bash]
----
kubectl apply -f ./hazelcast-persistence-hostpath.yaml
----
